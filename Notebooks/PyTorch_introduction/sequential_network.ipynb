{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn as nn \n",
    "from torch.optim import SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving our previous rudimentary network \n",
    "\n",
    "We defined a network based on a function that multiplies the input by two matrices $A_1$ and $A_2$. \n",
    "\n",
    "$$ \\text{Old Model: } \\textcolor{red}{\\boxed{f(x) = A_2 \\cdot A_1 \\cdot x}} $$ \n",
    "\n",
    "However, despite choosing $8 \\times 2$ and $8 \\times 1$ matrices, their resulting product is a $2 \\times 1$ matrix, which has a very small number of total parameters. \n",
    "\n",
    "So in reality, we ended up having \n",
    "\n",
    "$$ \\text{Old Model: } f(x) = B \\cdot x $$ \n",
    "\n",
    "where $B$ is a $2 \\times 1$ matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve upon this by defining the new model as such:\n",
    "\n",
    "$$ \\text{New Model:  } \\textcolor{green}{\\boxed{f(x) = A_2 \\cdot R (A_1 \\cdot x)}} $$\n",
    "\n",
    "where $R$ is an element-wise operator defined by \n",
    "\n",
    "$$ R(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Crux** of Machine Learning lies in so-called **Activation Functions**, which add subtle non-linearities to a sequence of matrix transformations. \n",
    "\n",
    "In the above example, $R$ is an identity function for positive values, and a function that clamps negative values to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes our activation function **close** to being a linear operator, but not quite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4,  6,  2, -1,  6,  2,  5],\n",
       "         [ 1,  6,  2, -6,  5, -3,  5]]),\n",
       " torch.Size([2, 7]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[4,6,2,-1,6,2,5], [1,6,2,-6,5,-3,5]])\n",
    "x, x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R$ function is called a **Rectified Linear Unit** or **ReLU** for short, and PyTorch has a built-in implementation of it, very conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4, 6, 2, 0, 6, 2, 5],\n",
       "         [1, 6, 2, 0, 5, 0, 5]]),\n",
       " torch.Size([2, 7]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = nn.ReLU()\n",
    "R(x), R(x).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-3, 3, 1000)\n",
    "y = R(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"394.423125pt\" height=\"297.190125pt\" viewBox=\"0 0 394.423125 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-03-06T20:28:13.705103</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 297.190125 \nL 394.423125 297.190125 \nL 394.423125 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 273.312 \nL 387.223125 273.312 \nL 387.223125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 46.335852 273.312 \nL 46.335852 7.2 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m34470b71c7\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m34470b71c7\" x=\"46.335852\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −3 -->\n      <g transform=\"translate(38.964759 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 100.444943 273.312 \nL 100.444943 7.2 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m34470b71c7\" x=\"100.444943\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −2 -->\n      <g transform=\"translate(93.073849 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 154.554034 273.312 \nL 154.554034 7.2 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m34470b71c7\" x=\"154.554034\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −1 -->\n      <g transform=\"translate(147.18294 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 208.663125 273.312 \nL 208.663125 7.2 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m34470b71c7\" x=\"208.663125\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(205.481875 287.910437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 262.772216 273.312 \nL 262.772216 7.2 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m34470b71c7\" x=\"262.772216\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(259.590966 287.910437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path d=\"M 316.881307 273.312 \nL 316.881307 7.2 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m34470b71c7\" x=\"316.881307\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2 -->\n      <g transform=\"translate(313.700057 287.910437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path d=\"M 370.990398 273.312 \nL 370.990398 7.2 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m34470b71c7\" x=\"370.990398\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 3 -->\n      <g transform=\"translate(367.809148 287.910437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path d=\"M 30.103125 261.216 \nL 387.223125 261.216 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path id=\"m52b011030a\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m52b011030a\" x=\"30.103125\" y=\"261.216\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 265.015219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path d=\"M 30.103125 220.896 \nL 387.223125 220.896 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m52b011030a\" x=\"30.103125\" y=\"220.896\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 224.695219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path d=\"M 30.103125 180.576 \nL 387.223125 180.576 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m52b011030a\" x=\"30.103125\" y=\"180.576\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 184.375219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path d=\"M 30.103125 140.256 \nL 387.223125 140.256 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#m52b011030a\" x=\"30.103125\" y=\"140.256\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.5 -->\n      <g transform=\"translate(7.2 144.055219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path d=\"M 30.103125 99.936 \nL 387.223125 99.936 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#m52b011030a\" x=\"30.103125\" y=\"99.936\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 103.735219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path d=\"M 30.103125 59.616 \nL 387.223125 59.616 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m52b011030a\" x=\"30.103125\" y=\"59.616\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2.5 -->\n      <g transform=\"translate(7.2 63.415219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path d=\"M 30.103125 19.296 \nL 387.223125 19.296 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#m52b011030a\" x=\"30.103125\" y=\"19.296\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 3.0 -->\n      <g transform=\"translate(7.2 23.095219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 46.335852 261.216 \nL 208.500639 261.216 \nL 208.825619 260.973832 \nL 370.990398 19.296 \nL 370.990398 19.296 \n\" clip-path=\"url(#pe688a7709f)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 273.312 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 387.223125 273.312 \nL 387.223125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 273.312 \nL 387.223125 273.312 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 387.223125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe688a7709f\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x.numpy(), y.numpy())\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this slightly more complex model compare to the previous one?\n",
    "\n",
    "Let's create the new Network and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_ReLU_network(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2, 8, bias=False)\n",
    "        self.Matrix2 = nn.Linear(8, 1, bias=False)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self, x): \n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the new model, but first let's write a function to do so, as it will come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, y, f, n_epochs, learning_rate):\n",
    "    opt = SGD(f.parameters(), lr=learning_rate)\n",
    "    L = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(n_epochs):\n",
    "        opt.zero_grad() # flush the previous epoch's gradient, or it would cumulate\n",
    "        loss_value = L(f(x), y) # compute loss \n",
    "        loss_value.backward() # compute the gradient\n",
    "        opt.step() # perform the iteration using the gradient above \n",
    "        losses.append(loss_value.item())\n",
    "    return f, losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same values we used for the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6, 2], [5, 2], [1, 3], [7, 6]]).float()\n",
    "y = torch.tensor([1, 5, 2, 5]).float()\n",
    "fun = my_ReLU_network()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also train for the same number of epochs and use the same learning rate, just to make sure we are comparing apples to apples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, losses = train_model(x, y, fun, n_epochs=50, learning_rate=0.001)\n",
    "yhat = f(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 5., 2., 5.]),\n",
       " tensor([2.6278, 2.3746, 1.9506, 5.0980], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still not great, but the predictions are a bit more accurate than before. However, this is not the end of the story. \n",
    "\n",
    "The real advantage of using this **Activation Function** is that we can use much larger matrices in the first place, whereas the previous model would collapse our parameters to a very small matrix regardless of the size of the input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this into practice by redefining the class to use $A_2$ as $1 \\times 80$ and $A_1$ as $80 \\times 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_ReLU_network_size(nn.Module): \n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2, dim, bias=False)\n",
    "        self.Matrix2 = nn.Linear(dim, 1, bias=False)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self, x): \n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6, 2], [5, 2], [1, 3], [7, 6]]).float()\n",
    "y = torch.tensor([1, 5, 2, 5]).float()\n",
    "fun = my_ReLU_network_size(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, losses = train_model(x, y, fun, 50, learning_rate=0.001)\n",
    "yhat = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 5., 2., 5.]),\n",
       " tensor([2.8100, 2.5250, 1.9061, 5.2396], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also train for a very large number of epochs and let's play with the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 5., 2., 5.]),\n",
       " tensor([1.6963, 3.9989, 1.9181, 5.1362], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[6, 2], [5, 2], [1, 3], [7, 6]]).float()\n",
    "y = torch.tensor([1, 5, 2, 5]).float()\n",
    "fun = my_ReLU_network_size(80)\n",
    "\n",
    "f, losses = train_model(x, y, fun, 5000, learning_rate=0.001)\n",
    "yhat = f(x)\n",
    "\n",
    "y, yhat\n",
    "\n",
    "# Quick note: this actually converges to the ideal result with a size 80 for the matrices, 5000 epochs and a larger learning rate of 0.01,\n",
    "# it seems to get not too close sometimes and occasionally it converges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still improve upon our model by introducing other parameters: \n",
    "\n",
    "\n",
    "$$ \\textcolor{red}{\\boxed{\\text{First Model: } f(x) = A_2 \\cdot A_1 \\cdot x}} $$\n",
    "\n",
    "$$ \\textcolor{yellow}{\\boxed{\\text{Previous Model: } f(x) = A_2 \\cdot R (A_1 \\cdot x)}} $$\n",
    "\n",
    "$$ \\textcolor{green}{\\boxed{\\text{New Model: } f(x) = A_2 \\cdot R (A_1 \\cdot x + b_1) + b_2}} $$\n",
    "\n",
    "where $b_1$ and $b_2$ are vectors added to each of the linear transformations, commonly known as **bias** parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU_biased_network(nn.Module): \n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2, dim, bias=True)\n",
    "        self.Matrix2 = nn.Linear(dim, 1, bias=True)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self, x): \n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()\n",
    "    \n",
    "# bias=True could be omitted as it's the default value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrain with our biased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 5., 2., 5.]),\n",
       " tensor([1.3814, 4.5234, 2.0195, 5.0155], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[6, 2], [5, 2], [1, 3], [7, 6]]).float()\n",
    "y = torch.tensor([1, 5, 2, 5]).float()\n",
    "fun = ReLU_biased_network(80)\n",
    "\n",
    "f, losses = train_model(x, y, fun, 5000, learning_rate=0.001)\n",
    "yhat = f(x)\n",
    "\n",
    "y, yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closer! But still not there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we added another matrix in the middle? Let's make one final adjustment to our model by introducing an $80 \\times 80$ matrix in the middle and the corresponding bias vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textcolor{red}{\\boxed{\\text{First Model: } f(x) = A_2 \\cdot A_1 \\cdot x}} $$\n",
    "\n",
    "$$ \\textcolor{orange}{\\boxed{\\text{Previous Model 1: } f(x) = A_2 \\cdot R (A_1 \\cdot x)}} $$\n",
    "\n",
    "$$ \\textcolor{yellow}{\\boxed{\\text{Previous Model 2: } f(x) = A_2 \\cdot R (A_1 \\cdot x + b_1) + b_2}} $$\n",
    "\n",
    "$$ \\textcolor{green}{\\boxed{\\text{New Model: } f(x) = A_3 \\cdot R (A_2 \\cdot R (A_1 \\cdot x + b_1) + b_2) + b_3}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3_ReLU_biased_network(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2, dim, bias=True)\n",
    "        self.Matrix2 = nn.Linear(dim, dim, bias=True)\n",
    "        self.Matrix3 = nn.Linear(dim, 1, bias=True)\n",
    "        self.R = nn.ReLU()\n",
    "    def forward(self, x): \n",
    "        x = self.R(self.Matrix1(x))\n",
    "        x = self.R(self.Matrix2(x))\n",
    "        x = self.Matrix3(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 5., 2., 5.]),\n",
       " tensor([1.0000, 5.0000, 2.0000, 5.0000], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[6, 2], [5, 2], [1, 3], [7, 6]]).float()\n",
    "y = torch.tensor([1, 5, 2, 5]).float()\n",
    "fun = M3_ReLU_biased_network(15)\n",
    "\n",
    "f, losses = train_model(x, y, fun, 5000, learning_rate=0.01)\n",
    "yhat = f(x)\n",
    "\n",
    "y, yhat\n",
    "\n",
    "# Note that this actually converges all the time with a size of 80 (as low as 15!), 5000 epochs and a larger learning rate of 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally got some pretty close predictions!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sequential Neural Network\n",
    "\n",
    "All of this leads us to the **Sequential Neural Network**, which is a very common type of network that is used in practice. \n",
    "\n",
    "It's simply a generalization of the previous model, where we can add as many layers as we want, and each layer can have as many parameters as we want. \n",
    "\n",
    "A general Sequential Neural Network is defined as such: \n",
    "\n",
    "$$ \\textcolor{green}{\\boxed{f(x) = \\displaystyle {\\Huge{\\kappa}}_{i=1}^n R_i (A_i \\cdot x + b_i)}}$$\n",
    "\n",
    "where ${\\Huge{\\kappa}}_{i=1}^n = f_n \\circ f_{n-1} \\circ \\cdots \\circ f_1(x)$ is an operator that chains functions, $A_i$ are matrices, $b_i$ are **bias** vectors, and $R_i$ are **activation functions**, typically **ReLU** for all layers except the last one, where it's usually just an **identity** function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Note**: In clever architectures, like convolutional neural networks, the matrices $A_i$ are not just regular matrices, but rather sparse matrices that are optimized for the specific task at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2c9d14e2283efc71dd845cb84a05c6dc0b4f1a825c12450a6974e74c8800d0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
