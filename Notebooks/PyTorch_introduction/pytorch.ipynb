{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using my base anaconda environment for this notebook, as opposed to the python runtimes I usually use for my other notebooks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy vs Torch\n",
    "\n",
    "Numpy **arrays** and PyTorch **tensors** are similar in many ways. They both are n-dimensional arrays, and they both support a large number of operations. However, there are some important differences between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0, 1, 5)\n",
    "t = torch.linspace(0, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be resized very similarly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(48).reshape(3, 4, 4)\n",
    "t = torch.arange(48).reshape(3, 4, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They have the same broadcasting rules."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Broadcasting Rules\n",
    "\n",
    "**NumPy** compares the shape of the two arrays element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when \n",
    "\n",
    "1. they are equal, or\n",
    "2. one of them is 1\n",
    "\n",
    "**Example**: The following arrays are compatible for broadcasting:\n",
    "\n",
    "    Shape1: (1, 6, 4, 1, 7, 2)\n",
    "    Shape2: (5, 6, 1, 3, 1, 2)\n",
    "\n",
    "Notice the numbers in parentheses are not actual elements of the array, but rather the shape of the array (they're both 6-dimensional arrays). The trailing dimensions are compatible because they are equal. The next two dimensions are compatible because one of them is 1. \n",
    "Going forward and checking element-wise, the two arrays are compatible for broadcasting.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((6, 5))\n",
    "b = np.arange(5).reshape((1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5)\n",
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're compatible! The trailing dimensions are equal, and the next two dimensions are compatible because one of them is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b$ has been broadcasted to match the shape of $a$, notice how it has been added elementwise and for each of the 5 rows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these examples work the same way using PyTorch tensors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Scaling each of the color channels of an image by a different amount \n",
    "\n",
    "        Image  (3D array): 256 x 256 x 3\n",
    "        Scale  (1D array):             3\n",
    "        Result (3D array): 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = torch.randn((256, 256, 3))\n",
    "Scale = torch.tensor([0.5, 1.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = Image * Scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel has its own RGB value and the scale array is broadcasted by automatically \"filling\" the missing dimensions with 1, which is compatible with the 3D array, as we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.4193e-01, -1.4909e-01,  3.3715e-01],\n",
       "         [ 2.0500e-01,  7.0912e-01, -3.8141e-01],\n",
       "         [-1.3512e-01, -4.9861e-01, -7.1900e-01],\n",
       "         ...,\n",
       "         [-6.9954e-02,  8.6427e-01, -1.6852e-02],\n",
       "         [-4.0529e-01,  1.6589e+00,  1.3983e+00],\n",
       "         [ 1.6758e-01,  6.9921e-01, -8.7392e-01]],\n",
       "\n",
       "        [[ 5.5269e-02,  7.1054e-01,  2.1175e-01],\n",
       "         [ 5.6743e-01, -1.0897e+00, -1.5861e-01],\n",
       "         [-6.7587e-01, -1.9006e+00,  1.8689e+00],\n",
       "         ...,\n",
       "         [ 2.5398e-01, -2.9557e+00, -6.9693e-01],\n",
       "         [ 3.2977e-01, -3.0399e-01,  5.4555e-01],\n",
       "         [ 4.7734e-01,  9.1462e-01, -1.9316e-01]],\n",
       "\n",
       "        [[ 3.2972e-01,  3.1169e+00, -1.4797e+00],\n",
       "         [ 2.7645e-01, -2.6284e+00,  4.2197e-02],\n",
       "         [ 3.9353e-02, -1.1383e-01,  1.1930e+00],\n",
       "         ...,\n",
       "         [-1.3341e-02, -1.2807e+00, -2.9090e-01],\n",
       "         [ 4.8910e-01, -2.0508e-03,  5.8567e-02],\n",
       "         [ 2.6874e-01,  2.3481e+00, -3.0073e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-5.0779e-01, -1.5866e+00, -9.5216e-01],\n",
       "         [-1.8664e-01, -1.6182e+00, -5.9005e-01],\n",
       "         [ 8.1741e-02,  9.2983e-02, -2.9879e-01],\n",
       "         ...,\n",
       "         [-5.1050e-01,  2.7961e-01, -2.4009e-01],\n",
       "         [ 1.7182e-01, -2.7771e+00, -2.6906e-01],\n",
       "         [-7.9407e-02, -4.2480e-01, -1.1797e+00]],\n",
       "\n",
       "        [[ 2.1657e-01, -9.3101e-01, -2.3013e+00],\n",
       "         [-2.0621e-01, -3.8360e-01, -1.4236e-02],\n",
       "         [ 9.3232e-01, -2.3103e+00,  5.3750e-01],\n",
       "         ...,\n",
       "         [-4.0344e-01,  3.4474e+00, -1.0796e+00],\n",
       "         [-2.4469e-01, -1.0435e+00,  6.3515e-01],\n",
       "         [-6.4339e-01, -3.1923e-01,  2.4976e-01]],\n",
       "\n",
       "        [[-1.0778e-01,  2.1940e+00,  6.7259e-01],\n",
       "         [-7.0818e-03,  1.0559e+00,  1.4888e+00],\n",
       "         [ 2.8383e-01,  1.6386e+00, -6.5101e-01],\n",
       "         ...,\n",
       "         [ 5.3852e-01, -1.0041e+00, -1.2551e+00],\n",
       "         [-3.2329e-01,  3.3761e-01,  1.5190e-01],\n",
       "         [ 2.3792e-02,  9.2187e-01,  9.1994e-01]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Suppose you have an array of 2 images and you want to scale the color channels of each image by a slightly different amount. \n",
    "\n",
    "        Image   (4D array):  2 x 256 x 256 x 3\n",
    "        Scale   (2D array):  2 x 1   x 1   x 3\n",
    "        Results (4D array):  2 x 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = torch.randn((2, 256, 256, 3))\n",
    "Scales = torch.tensor([0.5, 1.5, 1, 1.5, 1, 0.5]).reshape((2, 1, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = Images * Scales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2608, -0.1968, -1.2438],\n",
       "          [ 0.2564,  0.8337,  1.7528],\n",
       "          [-0.2752, -3.8720,  0.1174],\n",
       "          ...,\n",
       "          [-0.9657,  1.7531,  0.2916],\n",
       "          [ 0.9899,  0.3650,  0.5422],\n",
       "          [ 0.5137, -1.1284, -0.4637]],\n",
       "\n",
       "         [[-0.2700, -1.0392, -0.6198],\n",
       "          [ 0.2428,  0.5081,  0.5560],\n",
       "          [-0.4982, -0.0401,  0.1227],\n",
       "          ...,\n",
       "          [-0.6139, -1.1468,  0.4519],\n",
       "          [-0.3639, -0.2186,  0.2458],\n",
       "          [ 0.2020, -0.1111,  1.2536]],\n",
       "\n",
       "         [[ 0.1925,  0.0419,  0.7937],\n",
       "          [ 0.1874, -0.4274,  1.5424],\n",
       "          [ 0.2707,  0.8403,  0.7752],\n",
       "          ...,\n",
       "          [-0.7021, -2.3563, -1.1848],\n",
       "          [-0.3176, -0.6505,  1.6346],\n",
       "          [-0.2568,  0.5206,  0.8773]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1045, -1.0587, -0.6398],\n",
       "          [ 0.1956, -1.0350, -0.3277],\n",
       "          [-0.3620,  0.2435,  0.2397],\n",
       "          ...,\n",
       "          [ 0.9061, -0.9410,  0.7956],\n",
       "          [-0.6431, -2.4170, -1.1437],\n",
       "          [ 0.4827,  0.1014,  1.2664]],\n",
       "\n",
       "         [[ 0.1630, -0.9194, -1.7362],\n",
       "          [ 0.3743,  1.6840, -0.5780],\n",
       "          [ 0.3186, -0.5714,  0.1447],\n",
       "          ...,\n",
       "          [ 0.1412, -0.6407,  1.5058],\n",
       "          [ 0.6674,  1.0644,  0.8186],\n",
       "          [ 0.7152,  0.3882,  0.2131]],\n",
       "\n",
       "         [[-0.0512,  1.3358, -0.2809],\n",
       "          [ 0.0291, -1.0845,  2.4705],\n",
       "          [ 0.1316,  0.1758, -2.2768],\n",
       "          ...,\n",
       "          [ 0.2332, -0.4467,  0.4361],\n",
       "          [-0.2515, -0.0384,  1.5291],\n",
       "          [ 0.4176, -0.9593,  1.1834]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1349, -0.1269, -0.2187],\n",
       "          [-2.6082, -0.0789, -0.0099],\n",
       "          [-0.6274,  0.2303,  0.8793],\n",
       "          ...,\n",
       "          [ 0.7564,  1.6404,  0.2429],\n",
       "          [ 0.4135, -0.7025, -0.1958],\n",
       "          [-0.2375,  0.6690,  0.1526]],\n",
       "\n",
       "         [[ 0.5542,  1.4993, -0.3107],\n",
       "          [ 0.0055,  1.0783,  0.2597],\n",
       "          [-1.1422,  1.5483, -0.3172],\n",
       "          ...,\n",
       "          [-0.8880,  0.4005, -0.1670],\n",
       "          [ 0.5562,  0.7222,  0.6744],\n",
       "          [ 0.7622, -1.4146,  0.8633]],\n",
       "\n",
       "         [[ 1.0482,  0.3127,  0.6477],\n",
       "          [-1.0013,  0.1729,  0.3598],\n",
       "          [ 1.3550, -0.8796,  0.1074],\n",
       "          ...,\n",
       "          [ 2.5359,  1.0842, -0.3137],\n",
       "          [-0.9414,  0.3294,  0.4195],\n",
       "          [-0.6962,  0.3668,  0.2184]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.6157, -0.5181, -0.3405],\n",
       "          [-1.9643, -0.3691,  0.1114],\n",
       "          [-0.5508,  0.4012,  0.1930],\n",
       "          ...,\n",
       "          [-1.1048,  0.3158, -0.3242],\n",
       "          [-0.7707, -0.5773,  0.4518],\n",
       "          [ 2.7531,  1.0667, -0.8870]],\n",
       "\n",
       "         [[-0.4794,  1.1747,  0.0539],\n",
       "          [-2.2979,  0.8359,  0.0249],\n",
       "          [ 1.6207,  0.9901,  0.2034],\n",
       "          ...,\n",
       "          [ 1.1884,  1.5346,  1.5539],\n",
       "          [ 0.5387,  1.1232,  0.9215],\n",
       "          [-0.4949,  1.2817,  0.7068]],\n",
       "\n",
       "         [[-0.6108, -1.4578, -0.7007],\n",
       "          [-2.3778, -0.7036,  0.5598],\n",
       "          [ 0.4871,  0.0300, -0.1763],\n",
       "          ...,\n",
       "          [-1.9421,  0.4537, -0.9466],\n",
       "          [-1.7810, -1.7627,  0.5224],\n",
       "          [-0.4838,  0.0814, -0.0749]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this scenario, the array would not be broadcasted correctly, as *PyTorch* would fill all the dimensions with 1s, whereas we needed to have the first dimension as 2.\n",
    "\n",
    "In this kind of situations, we utilized the `reshape` subroutine to serve our needs. \n",
    "\n",
    "It is always important to spend some time thinking about the shapes of the arrays you are working with, and to make sure that the operations you are performing are valid."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations Across Dimensions\n",
    "\n",
    "This is a fundamental aspect of **PyTorch**. Let's see first a one dimensional tensor, which is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1250), tensor(1.6520), tensor(4.), tensor(0.5000))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0.5, 1, 3, 4])\n",
    "torch.mean(t), torch.std(t), torch.max(t), torch.min(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a 2D tensor instead and we want to compute the mean of each column. This means taking the mean of each dimension, so it's an operation across dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]], dtype=torch.float64),\n",
       " tensor([ 8.,  9., 10., 11.], dtype=torch.float64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(20, dtype=float).reshape(5, 4)\n",
    "t, torch.mean(t, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be of course generalized for higher dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(5, 256, 256, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could represent a batch of 5 images, with resolution of 256 x 256 pixels, and 3 color channels per pixel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean across all the images, so we get a single image with the mean of each pixel across all the images, including the color values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3562, -0.4056,  0.2154],\n",
       "          [ 0.5333, -0.0079,  0.2167],\n",
       "          [-0.5909,  0.4377, -0.1056],\n",
       "          ...,\n",
       "          [-0.1916,  0.2542, -0.4962],\n",
       "          [ 0.0158,  0.0699,  0.1010],\n",
       "          [ 0.3502,  0.4203, -0.0420]],\n",
       " \n",
       "         [[ 0.0630, -0.1332, -0.8394],\n",
       "          [ 0.6533,  0.2290, -0.5519],\n",
       "          [ 0.7248, -0.5055,  0.0945],\n",
       "          ...,\n",
       "          [-0.0319,  0.0365,  0.2041],\n",
       "          [-0.3483,  0.1173,  0.2313],\n",
       "          [ 0.1733, -0.2464, -0.7887]],\n",
       " \n",
       "         [[-0.1424,  0.1337, -0.3838],\n",
       "          [-0.8562,  0.0780,  0.3785],\n",
       "          [ 0.3333, -1.1031, -0.0909],\n",
       "          ...,\n",
       "          [-0.0325, -0.8204, -0.2991],\n",
       "          [-0.4003,  0.9888,  0.0232],\n",
       "          [ 0.2096, -0.5949, -0.6839]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.8043, -0.2688, -0.1386],\n",
       "          [ 0.5974,  0.0413,  0.4535],\n",
       "          [-0.4871,  0.7350,  0.0976],\n",
       "          ...,\n",
       "          [ 0.5585, -0.3826, -0.6759],\n",
       "          [-0.3808,  0.4308, -0.5710],\n",
       "          [ 0.3927,  0.1967, -1.0629]],\n",
       " \n",
       "         [[ 0.2554, -0.7134, -0.0341],\n",
       "          [ 0.5654,  0.3781,  0.1560],\n",
       "          [ 0.3822,  0.2593, -0.2710],\n",
       "          ...,\n",
       "          [-0.2627,  0.4113,  0.3179],\n",
       "          [ 0.0884,  0.5027, -0.1698],\n",
       "          [-0.7186, -0.8353, -0.2134]],\n",
       " \n",
       "         [[-0.6705, -0.0777,  0.3226],\n",
       "          [-0.5853, -0.5159,  0.1868],\n",
       "          [ 0.1507,  1.2077, -0.1983],\n",
       "          ...,\n",
       "          [ 0.1327,  0.2532, -0.1266],\n",
       "          [ 0.3680, -0.2710, -0.4202],\n",
       "          [ 0.0605,  0.2827, -0.5337]]]),\n",
       " torch.Size([256, 256, 3]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=0), torch.mean(t, axis=0).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean across the color channels only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.5468,  0.1881,  0.4608,  ..., -0.2062,  0.1819,  0.3658],\n",
       "          [ 0.3465,  0.0852,  0.3815,  ...,  0.1111, -0.0526, -0.0725],\n",
       "          [-0.2049,  0.8760, -0.5861,  ..., -0.0394, -0.0922, -0.8019],\n",
       "          ...,\n",
       "          [-0.0092, -0.0455,  0.4595,  ..., -0.5586, -0.2606,  0.1232],\n",
       "          [-0.1643,  0.2186,  0.2978,  ..., -0.7592, -0.5829, -0.1258],\n",
       "          [ 0.0830, -0.7184,  0.0985,  ..., -0.8119, -0.0943,  0.3497]],\n",
       " \n",
       "         [[-0.6953, -0.0172, -0.7987,  ..., -0.6359,  0.7798, -0.0479],\n",
       "          [ 0.0264,  0.6073, -0.0743,  ..., -0.1566, -0.0672, -0.4779],\n",
       "          [ 0.2467,  0.3761, -0.2785,  ..., -0.7459,  0.6505, -0.4954],\n",
       "          ...,\n",
       "          [ 0.4924,  0.3945, -0.2559,  ..., -0.1754,  0.7891, -0.0143],\n",
       "          [ 0.1805,  0.0310, -0.1723,  ...,  0.8701,  0.5645, -0.7392],\n",
       "          [ 0.0541,  0.7855,  0.0832,  ...,  0.5471,  0.7042, -0.6264]],\n",
       " \n",
       "         [[-0.4218,  0.1707,  0.1059,  ...,  0.0957, -0.1982,  0.4580],\n",
       "          [-0.3592,  0.1770,  0.6395,  ...,  0.4703, -0.1278,  0.7145],\n",
       "          [-0.6299, -0.3275, -0.6010,  ..., -0.6461,  0.6311,  0.0089],\n",
       "          ...,\n",
       "          [-0.7588,  0.3261, -0.0128,  ...,  0.4841, -0.0097, -0.0336],\n",
       "          [-0.3439,  0.3521, -0.1224,  ..., -0.0669,  0.6839, -0.3109],\n",
       "          [-0.4525,  0.0075,  0.6108,  ..., -0.4547, -0.5108,  0.2730]],\n",
       " \n",
       "         [[-0.9912,  0.3039, -0.4074,  ...,  0.0651,  0.1147, -0.0942],\n",
       "          [-0.4999,  0.2663, -0.2668,  ..., -0.1692,  0.6062, -0.6983],\n",
       "          [-0.0673, -1.2009,  0.3444,  ..., -0.3096, -0.6588,  0.0801],\n",
       "          ...,\n",
       "          [ 0.1520,  0.7775, -0.0240,  ...,  0.0240, -0.9836, -0.4422],\n",
       "          [-0.6643,  0.4521,  0.1109,  ..., -0.2265, -0.5531, -1.1114],\n",
       "          [ 0.0451, -0.8170,  0.6729,  ...,  0.5300, -0.8166,  0.0113]],\n",
       " \n",
       "         [[ 0.6507,  0.5913,  0.2082,  ..., -0.0415, -0.5670,  0.5326],\n",
       "          [-1.0298, -0.5853, -0.1569,  ...,  0.0922, -0.3580, -0.9020],\n",
       "          [ 0.0012, -0.3902, -0.3131,  ..., -0.1790,  0.4888, -0.5737],\n",
       "          ...,\n",
       "          [ 0.7851,  0.3676,  0.4089,  ..., -0.6075, -0.4036, -0.4223],\n",
       "          [ 0.1716,  0.7786,  0.5035,  ...,  0.9601,  0.5898, -0.6581],\n",
       "          [-0.4390, -0.7815,  0.4682,  ...,  0.6218,  0.1789, -0.3252]]]),\n",
       " torch.Size([5, 256, 256]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=-1), torch.mean(t, axis=-1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only the maximum color channel values and the corresponging indices:\n",
    "\n",
    "+ This is done all the time in image segmentation models, where we can isolate the pixels that belong to a certain class of object for which an algorithm has been trained, say a car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0671e+00,  6.8944e-01,  9.5293e-01,  ...,  1.3474e+00,\n",
       "            1.0383e+00,  7.7519e-01],\n",
       "          [ 9.4600e-01,  1.7104e+00,  9.9561e-01,  ...,  6.1847e-01,\n",
       "            6.1800e-01,  1.5198e+00],\n",
       "          [ 2.3456e-01,  1.9537e+00,  1.5769e-01,  ...,  1.4087e+00,\n",
       "            1.2904e+00, -2.6715e-01],\n",
       "          ...,\n",
       "          [ 5.6467e-01,  1.7992e-01,  1.4905e+00,  ...,  7.7292e-01,\n",
       "            8.6508e-01,  8.2257e-01],\n",
       "          [ 3.6003e-01,  1.3207e+00,  1.0240e+00,  ...,  2.8150e-01,\n",
       "            6.1890e-01,  3.4369e-01],\n",
       "          [ 1.6209e+00,  2.0949e-01,  1.2307e+00,  ..., -5.9271e-02,\n",
       "            8.9028e-01,  1.6284e+00]],\n",
       " \n",
       "         [[-9.1576e-02,  5.3377e-01,  1.7418e-01,  ...,  8.6056e-01,\n",
       "            1.2178e+00,  9.7631e-01],\n",
       "          [ 4.4572e-01,  1.2215e+00,  5.0402e-01,  ...,  3.6234e-01,\n",
       "            8.9612e-01,  4.6493e-02],\n",
       "          [ 8.4660e-01,  1.9829e+00,  5.7429e-01,  ...,  1.3984e-02,\n",
       "            1.5751e+00,  1.4441e+00],\n",
       "          ...,\n",
       "          [ 9.7275e-01,  6.6987e-01,  6.5293e-01,  ...,  5.0462e-01,\n",
       "            1.5089e+00,  1.0418e+00],\n",
       "          [ 2.0303e+00,  3.9595e-01,  1.5235e+00,  ...,  1.6863e+00,\n",
       "            1.3643e+00,  5.3642e-02],\n",
       "          [ 6.5721e-01,  1.6557e+00,  9.0061e-01,  ...,  1.5698e+00,\n",
       "            8.2403e-01,  9.2111e-01]],\n",
       " \n",
       "         [[ 6.5375e-02,  1.2207e+00,  7.1425e-01,  ...,  5.5344e-01,\n",
       "            8.8519e-02,  5.9058e-01],\n",
       "          [ 1.3929e-01,  2.5402e-01,  1.6101e+00,  ...,  8.6199e-01,\n",
       "            8.4759e-01,  1.5927e+00],\n",
       "          [-2.2864e-01,  4.9167e-01,  5.5826e-01,  ..., -1.3457e-01,\n",
       "            1.3513e+00,  9.7791e-01],\n",
       "          ...,\n",
       "          [ 3.4256e-01,  9.9328e-01,  5.2515e-01,  ...,  1.9135e+00,\n",
       "            1.1235e+00,  1.3314e+00],\n",
       "          [ 9.5243e-01,  1.3292e+00,  2.6401e-01,  ...,  2.8835e-01,\n",
       "            1.2866e+00,  1.5676e+00],\n",
       "          [ 9.2125e-01,  1.6796e-01,  1.3535e+00,  ...,  2.7180e-01,\n",
       "            8.3719e-01,  1.2154e+00]],\n",
       " \n",
       "         [[-6.5803e-01,  7.1845e-01,  1.1583e-01,  ...,  7.6008e-01,\n",
       "            1.1984e+00,  8.6986e-01],\n",
       "          [-3.6900e-01,  1.8585e+00,  8.9781e-01,  ...,  1.1678e+00,\n",
       "            1.7523e+00,  2.2260e-04],\n",
       "          [ 1.2729e-01, -3.3553e-01,  8.6954e-01,  ...,  9.3696e-01,\n",
       "            1.0852e-01,  1.5282e+00],\n",
       "          ...,\n",
       "          [ 1.8516e+00,  1.3313e+00,  1.7480e+00,  ...,  8.7196e-01,\n",
       "           -6.2990e-01,  6.6336e-01],\n",
       "          [ 4.1367e-01,  1.0699e+00,  1.0038e+00,  ...,  8.5350e-01,\n",
       "            7.8338e-01, -1.8864e-01],\n",
       "          [ 5.4770e-01,  2.4333e-01,  2.5553e+00,  ...,  1.2300e+00,\n",
       "            2.7593e-01,  2.1505e-01]],\n",
       " \n",
       "         [[ 1.1267e+00,  2.3083e+00,  9.7024e-01,  ...,  4.4166e-01,\n",
       "           -3.5698e-01,  1.1802e+00],\n",
       "          [-5.8023e-01,  1.6169e-01,  1.1339e+00,  ...,  4.6557e-01,\n",
       "            1.6822e-02,  1.0452e+00],\n",
       "          [ 3.3197e-01,  2.3412e-01,  4.6478e-01,  ...,  2.4401e-01,\n",
       "            1.2577e+00,  1.1929e+00],\n",
       "          ...,\n",
       "          [ 1.6207e+00,  7.2191e-01,  1.0724e+00,  ..., -5.7018e-03,\n",
       "            4.5320e-01,  5.1688e-01],\n",
       "          [ 1.2259e+00,  1.5103e+00,  1.3436e+00,  ...,  1.1387e+00,\n",
       "            1.9610e+00, -2.8916e-01],\n",
       "          [ 9.5757e-01, -2.6164e-02,  8.7409e-01,  ...,  1.2292e+00,\n",
       "            1.1714e+00,  2.3624e-01]]]),\n",
       " tensor([[[2, 2, 0,  ..., 0, 1, 1],\n",
       "          [0, 0, 2,  ..., 2, 2, 1],\n",
       "          [1, 2, 0,  ..., 2, 1, 0],\n",
       "          ...,\n",
       "          [0, 1, 1,  ..., 0, 1, 0],\n",
       "          [1, 0, 1,  ..., 1, 1, 0],\n",
       "          [2, 2, 1,  ..., 1, 0, 0]],\n",
       " \n",
       "         [[2, 1, 2,  ..., 1, 2, 1],\n",
       "          [0, 0, 0,  ..., 1, 2, 1],\n",
       "          [2, 1, 0,  ..., 0, 1, 0],\n",
       "          ...,\n",
       "          [0, 2, 1,  ..., 1, 0, 1],\n",
       "          [0, 2, 0,  ..., 1, 2, 1],\n",
       "          [2, 1, 0,  ..., 2, 1, 1]],\n",
       " \n",
       "         [[0, 0, 1,  ..., 1, 1, 1],\n",
       "          [1, 2, 0,  ..., 1, 0, 2],\n",
       "          [0, 2, 0,  ..., 2, 1, 0],\n",
       "          ...,\n",
       "          [0, 2, 1,  ..., 0, 1, 0],\n",
       "          [2, 1, 2,  ..., 1, 0, 2],\n",
       "          [2, 1, 1,  ..., 0, 1, 2]],\n",
       " \n",
       "         [[2, 2, 1,  ..., 1, 2, 0],\n",
       "          [0, 1, 2,  ..., 0, 1, 2],\n",
       "          [0, 2, 1,  ..., 0, 2, 0],\n",
       "          ...,\n",
       "          [0, 0, 2,  ..., 0, 1, 1],\n",
       "          [2, 1, 2,  ..., 2, 1, 2],\n",
       "          [2, 2, 1,  ..., 1, 0, 1]],\n",
       " \n",
       "         [[2, 0, 1,  ..., 2, 0, 2],\n",
       "          [0, 0, 0,  ..., 1, 2, 0],\n",
       "          [1, 2, 2,  ..., 0, 1, 1],\n",
       "          ...,\n",
       "          [1, 0, 1,  ..., 0, 1, 0],\n",
       "          [2, 2, 1,  ..., 0, 2, 0],\n",
       "          [1, 0, 2,  ..., 0, 2, 0]]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = torch.max(t, axis=-1)\n",
    "\n",
    "values, indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and NumPy Differences\n",
    "\n",
    "So far we have seen that PyTorch tensors and NumPy arrays are very similar. However, there are some important differences between them.\n",
    "\n",
    "Computation of gradients of operations on tensors is done automatically, whereas in NumPy we have to do it manually.\n",
    "\n",
    "$$ y = \\sum_{i=1}^n x_i^3 $$ \n",
    "\n",
    "has a gradient of\n",
    "\n",
    "$$ \\displaystyle\\frac{\\partial y}{\\partial x_i} = 3 x_i^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[5., 8.], [4., 6.]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(917., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.pow(3).sum()\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the gradient using the analytical formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * x**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatic computation of gradients is very useful, as it allows us to easily implement many machine learning algorithms. The example above is a very simple one, but in practice we often have to compute the gradient of a very complex function, and it would be very tedious to do it manually. \n",
    "\n",
    "In general, by having \n",
    "\n",
    "$$ y = f(\\vec{x}) $$\n",
    "\n",
    "**PyTorch** can compute $\\partial y / \\partial \\vec{x_i}$ automatically for each $i$ in $\\vec{x}$.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very simple terms, a **neural network** is a function that takes an input and produces an output. The parameters of the **function** $f$ are called **weights**, stored in a tensor $\\vec{x}$. Gradients measure the change of these weights and it's important to compute them automatically, as it allows us to update the weights in the right direction, so that the function $f$ can be optimized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Benefits\n",
    "\n",
    "Another great benefit of **PyTorch** is that it calculates large matrix multiplications even more efficiently than **NumPy** does. This is even more accentuated when using a GPU, which is the main reason why **PyTorch** is so popular in the field of deep learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a quick trivial benchmark of matrix multiplication between **NumPy** and **PyTorch**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple example, but it shows the difference in speed between the two libraries. The results are even more dramatic as I'm using PyTorch on an *Nvidia RTX 2070 Super* GPU, with **CUDA 11.7**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(1e4)\n",
    "A = torch.randn((N, N))\n",
    "B = torch.randn((N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.979303668000057\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "torch.matmul(A, B)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(t2 - t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is achieved with a CPU computation. Now let's see how much faster it is when using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.cuda()\n",
    "B = B.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.023646036999935\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "torch.matmul(A, B)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(t2 - t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(1e4)\n",
    "A = np.random.randn(N**2).reshape((N, N))\n",
    "B = np.random.randn(N**2).reshape((N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56590535700002\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "A@B\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(t2 - t1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is quite remarkable, despite my GPU being a mid-range consumer card and my CPU being a 12 core Ryzen 9 3900X. The test could be repeated with larger matrices, but going up a whole order of magnitude would require even more memory than my 64GB of RAM, which is already quite a lot.\n",
    "\n",
    "One thing I've noticed is that **NumPy** warns you beforehand if you're going to run out of memory, whereas **PyTorch** doesn't. This is a very important feature, as it allows you to avoid running out of memory and crashing your program or your system.\n",
    "\n",
    "PyTorch actually uses more memory than NumPy in order to be able to compute the operations faster, which is why I did not generate them on the GPU directly, just in case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2c9d14e2283efc71dd845cb84a05c6dc0b4f1a825c12450a6974e74c8800d0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
