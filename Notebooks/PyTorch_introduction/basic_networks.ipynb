{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.optim import SGD \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concepts of Machine Learning\n",
    "\n",
    "Suppose you have an independent dataset $\\vec{x}$ and another dataset $\\vec{y}$ that is dependent on $\\vec{x}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of this are:\n",
    "\n",
    "+ $x_i$ is the height of a person and $y_i$ is the weight of the person, predicted by the height\n",
    "+ $x_i$ is a picture of a handwritten digit and $y_i$ is the digit that is written in the picture, so the number itself is predicted by its representation in the picture\n",
    "+ $x_i$ is a CT scan of a patient and $y_i$ are the pixels corresponding tumors in the scan, so the tumor is predicted certain pixel patterns in the image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a **neural network** is as follows. Define a function $f$ that depends on parameters $a$ that makes the predictions\n",
    "\n",
    "$$ \\hat{y}_i = f(x_i; a) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions $\\hat{y}_i$ and the true values $y_i$ shoudl be as close as possible by adjusting the parameters $a$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters $a$ are adjusted by a **loss function** $L$ that measures the difference between the predictions $\\hat{y}_i$ and the true values $y_i$. The loss function is defined as \n",
    "\n",
    "$$ L(y, \\hat{y}) = \\sum_{i=1}^N \\left( \\hat{y}_i - y_i \\right)^2 $$\n",
    "\n",
    "for an example as trivial as the first one above. \n",
    "\n",
    "In general, one has to clearly identify what should be compared in order to determine the similarity between the predictions and the true values, in order to properly design and define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6, 2], [5, 2], [1, 3], [7, 6]]).float()\n",
    "y = torch.tensor([1, 5, 2, 5]).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $x_1 = (6, 2), x_2 = (5, 2), \\dots$ \n",
    "+ $y_1 = 1, y_2 = 5, \\dots$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to find is a function $f$ that depends on parameters $a$ that maps the input $\\vec{x}$ to the output $\\vec{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[6., 2.],\n",
       "         [5., 2.],\n",
       "         [1., 3.],\n",
       "         [7., 6.]]),\n",
       " tensor([1., 5., 2., 5.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core idea of machine learning is to use as many data points as possible to find the best function $f$ that maps the input $\\vec{x}$ to the output $\\vec{y}$. \n",
    "\n",
    "**Idea**: \n",
    "\n",
    "1. First multiply each element in $\\vec{x}$ by a $8 \\times 2$ matrix (16 parameters $a_i$)\n",
    "2. Then multiply each element in $\\vec{x}$ by a $1 \\times 8$ matrix (8 parameters $a_i$)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a matrix (takes in a 2D vector and outputs a 8D vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: When the matrix is created, it is initalized with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=8, bias=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = nn.Linear(2, 8, bias=False) # this function takes in a 2D vector and returns an 8D vector (8 is just arbitrary for this example)\n",
    "M1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can pass to $M_1$ the vector $\\vec{x}$ (dataset), where each element $x_i$ is an instance of the dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a 2D vector, so I can pass it to $M_1$ and it will output a 8D vector by multiplying each element in $\\vec{x}$ by the matrix $M_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.2691,  5.2286, -0.2458, -4.4349, -3.8910,  2.6982,  1.2514, -3.5692],\n",
       "        [-4.6219,  4.5361, -0.1088, -3.8837, -3.3692,  2.3647,  0.8158, -3.1923],\n",
       "        [-2.7265,  2.3027,  0.7279, -2.2423, -1.6622,  1.3800, -1.6074, -2.3385],\n",
       "        [-8.6887,  8.0681,  0.7704, -7.2409, -5.9333,  4.4270, -1.0370, -6.5615]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now generate the new matrix $M_2$ that takes in a 8D vector and outputs a 1D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=1, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2 = nn.Linear(8, 1, bias=False)\n",
    "M2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I can chain the multiplication of the two matrices $M_1$ and $M_2$ to get the final output, as said in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2293, -0.2910, -0.8376, -1.3667], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2(M1(x)).squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions we found are not very close to the true values, so we need to adjust the parameters $a$ in order to minimize the loss function $L$. \n",
    "\n",
    "The two matrices $M_1$ and $M_2$ constitute the parameters $a$.\n",
    "\n",
    "What we need to do is to do is to **train** the *neural network*. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our custom network $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2, 8, bias=False)\n",
    "        self.Matrix2 = nn.Linear(8, 1, bias=False)\n",
    "    def forward(self, x):\n",
    "        x = self.Matrix1(x)\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a class is a good way to define a custom network, as you can conveniently store the parameters $a$ in the class, while inheriting from the `nn.Module` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = my_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2428, -0.6906],\n",
      "        [-0.5769,  0.0889],\n",
      "        [-0.0593, -0.6112],\n",
      "        [ 0.6794,  0.2053],\n",
      "        [-0.0832, -0.6082],\n",
      "        [-0.2675,  0.0695],\n",
      "        [-0.6002,  0.1153],\n",
      "        [ 0.1161,  0.6740]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0075,  0.2225, -0.0799,  0.2529,  0.1428, -0.3092,  0.1780, -0.1969]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for par in f.parameters():\n",
    "    print(par)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that these are the same matrices $M_1$ and $M_2$ that we defined before and that we can access using the `parameters()` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the dataset $\\vec{x}$ to the network $f$ and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2426, -0.2337, -0.2932, -0.6305], grad_fn=<SqueezeBackward0>),\n",
       " tensor([1., 5., 2., 5.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = f(x)\n",
    "yhat, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is working correctly, so we can now define the loss function $L$ and the optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting $a$ so that $\\vec{\\hat{y}}$ and $\\vec{y}$ are as close as possible"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the loss function $L$ as the mean squared error between the predictions $\\vec{\\hat{y}}$ and the true values $\\vec{y}$. The function is conveniently defined in the `torch.nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4743, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = nn.MSELoss()\n",
    "L(y, yhat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it is actually doing what we want, i.e. let's compute the regular mean squared error between the predictions $\\vec{\\hat{y}}$ and the true values $\\vec{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4743, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y - yhat)**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $L$ is a function of the parameters $a$, $L = L(a)$ (remember that it depends on $\\hat{y}$ which depends on $a$). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **key idea behind machine learning** is to compute \n",
    "\n",
    "$$ \\displaystyle\\frac{\\partial L}{\\partial a_i} $$\n",
    "\n",
    "for each parameter $a_i$ of the newtork $f$. Then we can adjust each parameter $a_i$ as follows\n",
    "\n",
    "$$ \\displaystyle a_i \\rightarrow a_i - \\eta \\frac{\\partial L}{\\partial a_i} $$\n",
    "\n",
    "where $\\eta$ is the **learning rate**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to do this over and over again, until the loss function $L$ is minimized. This is called **gradient descent**.\n",
    "\n",
    "+ Each pass of the full data set $\\vec{x}$ is called an **epoch**. \n",
    "\n",
    "The `SGD` optimizer (stochastic gradient descent) takes in all model parameters along with the learning rate $\\eta$ and performs the update of the parameters $a_i$ as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(f.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2c9d14e2283efc71dd845cb84a05c6dc0b4f1a825c12450a6974e74c8800d0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
